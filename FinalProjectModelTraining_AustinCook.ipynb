{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "from numpy import ndarray\n",
        "from typing import Tuple\n",
        "import csv\n",
        "from tabulate import tabulate"
      ],
      "metadata": {
        "id": "VTcota7809En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og8a_BsNz8lS"
      },
      "outputs": [],
      "source": [
        "COUNTRY, POP, CHRISTIAN = 0, 1, 2\n",
        "\n",
        "def load_music_genre_classification(n_samples=float('inf'), normalize=False) -> ndarray:\n",
        "  '''My custom dataset'''\n",
        "  with open('music_genre_classification.csv', 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    data = list(reader)\n",
        "\n",
        "    # convert to numpy array\n",
        "    data = np.array(data)\n",
        "\n",
        "    # extract X and y\n",
        "    X = data[1:, 4:-1]\n",
        "    y = data[1:, -1:]\n",
        "    X = X.astype(float)\n",
        "    y = np.where(y == 'country', COUNTRY, y)\n",
        "    y = np.where(y == 'pop', POP, y)\n",
        "    y = np.where(y == 'christian', CHRISTIAN, y)\n",
        "    y = y.astype(float)\n",
        "    y = y.flatten()\n",
        "\n",
        "    if normalize:\n",
        "      X = preprocessing.MinMaxScaler().fit_transform(X)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def softmax(clf, X_test: ndarray, y_test: ndarray):\n",
        "  probs = clf.predict_proba(X_test)\n",
        "  avg_certainty_correct_answer(probs, y_test)\n",
        "\n",
        "  print(\"A few of the softmax probabilities using test data:\")\n",
        "\n",
        "  for i in range(len(X_test)):\n",
        "    if i < 10:\n",
        "      print(f'{probs[i]} ({y_test[i]} actual)')\n",
        "\n",
        "def avg_certainty_correct_answer(probs: ndarray, y: ndarray) -> None:\n",
        "  # EX: probs[i]=[0.21255198 0.55361867 0.23382935]  y[i]=0.0 (actual)\n",
        "  assert(probs.shape[1] == 3)\n",
        "  assert(probs.shape[0] == len(y))\n",
        "  for i in y:\n",
        "    assert(i in [0, 1, 2])\n",
        "\n",
        "  avg_correct, avg_incorrect = 0, 0\n",
        "  for row in range(probs.shape[0]):\n",
        "    correct_class_i = int(y[row])\n",
        "\n",
        "    avg_correct += probs[row, correct_class_i]\n",
        "\n",
        "    for incorrect_class_i in [0, 1, 2]:\n",
        "      if incorrect_class_i != correct_class_i:\n",
        "        avg_incorrect += probs[row, incorrect_class_i]\n",
        "\n",
        "  avg_correct /= probs.shape[0]\n",
        "  avg_incorrect /= (probs.shape[0] * 2)\n",
        "\n",
        "  print(\"Avg certainty for given to correct vs each of the 2 incorrect classification:\")\n",
        "  print(f'   Correct: {avg_correct}, Incorrect: {avg_incorrect}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Perceptron - Initial\\n\")\n",
        "\n",
        "X, y = load_music_genre_classification(normalize=False)\n",
        "\n",
        "NUM_TRIALS = 10\n",
        "\n",
        "headers = [\"Trial\", \"Training Accuracy\", \"Test Accuracy\", \"Number of Epochs\"]\n",
        "table = []\n",
        "for i in range(NUM_TRIALS):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
        "  clf = Perceptron(random_state=i).fit(X_train, y_train)\n",
        "\n",
        "  table.append([i, clf.score(X_train, y_train), clf.score(X_test, y_test), clf.n_iter_])\n",
        "\n",
        "# add row of averages\n",
        "averages = [\"Average\", 0, 0, 0]\n",
        "for i in range(len(table)):\n",
        "  for j in range(1, 4):\n",
        "    averages[j] += table[i][j]\n",
        "for i in range(1, 4):\n",
        "  averages[i] /= len(table)\n",
        "table.append(averages)\n",
        "\n",
        "print(tabulate(table, headers=headers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hn8ba-O77hf0",
        "outputId": "c8241fbe-2c99-4577-decf-ad97f36696bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perceptron - Initial\n",
            "\n",
            "Trial      Training Accuracy    Test Accuracy    Number of Epochs\n",
            "-------  -------------------  ---------------  ------------------\n",
            "0                   0.466964         0.475                   11\n",
            "1                   0.468452         0.461905                17\n",
            "2                   0.455357         0.47381                 16\n",
            "3                   0.330357         0.344048                17\n",
            "4                   0.39881          0.403571                17\n",
            "5                   0.364286         0.369048                12\n",
            "6                   0.383929         0.377381                10\n",
            "7                   0.340774         0.320238                12\n",
            "8                   0.395833         0.380952                10\n",
            "9                   0.411012         0.414286                10\n",
            "Average             0.401577         0.402024                13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Perceptron - Improved\\n\")\n",
        "# ETA is basically the only thing\n",
        "\n",
        "X, y = load_music_genre_classification(normalize=True)\n",
        "\n",
        "headers = [\"ETA0\", \"Avg Training Accuracy\", \"Avg Test Accuracy\", \"Avg Number of Epochs\"]\n",
        "table = []\n",
        "\n",
        "NUM_TRIALS = 10\n",
        "\n",
        "eta0s = [0.001, 0.01, 0.1, 1, 10, 100000]\n",
        "for eta0 in eta0s:\n",
        "  row = [eta0, 0, 0, 0]\n",
        "  for i in range(NUM_TRIALS):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
        "    clf = Perceptron(random_state=i, eta0=eta0).fit(X_train, y_train)\n",
        "\n",
        "    row[1] += clf.score(X_train, y_train)\n",
        "    row[2] += clf.score(X_test, y_test)\n",
        "    row[3] += clf.n_iter_\n",
        "\n",
        "  table.append(row)\n",
        "\n",
        "for i in range(len(table)):\n",
        "  for j in range(1, 4):\n",
        "    table[i][j] /= NUM_TRIALS\n",
        "\n",
        "print(tabulate(table, headers=headers))"
      ],
      "metadata": {
        "id": "9Jz1Yp4d75o9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfd9f56d-1d20-49c4-a33e-7be8822a14f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perceptron - Improved\n",
            "\n",
            "      ETA0    Avg Training Accuracy    Avg Test Accuracy    Avg Number of Epochs\n",
            "----------  -----------------------  -------------------  ----------------------\n",
            "     0.001                 0.729107             0.729524                     6\n",
            "     0.01                  0.729107             0.729524                     6\n",
            "     0.1                   0.721905             0.724643                     6.2\n",
            "     1                     0.694048             0.692976                    13.5\n",
            "    10                     0.701815             0.699643                    13.7\n",
            "100000                     0.701815             0.699643                    13.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MLP - Initial\\n\")\n",
        "\n",
        "X, y = load_music_genre_classification(normalize=False)\n",
        "\n",
        "NUM_TRIALS = 10\n",
        "\n",
        "headers = [\"Trial\", \"Training Accuracy\", \"Test Accuracy\", \"Number of Iterations\"]\n",
        "table = []\n",
        "avg_n_iter, avg_train_acc, avg_test_acc = 0, 0, 0\n",
        "for i in range(NUM_TRIALS):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
        "  clf = MLPClassifier(random_state=i).fit(X_train, y_train)\n",
        "\n",
        "  n_iter = clf.n_iter_\n",
        "  train_acc = clf.score(X_train, y_train)\n",
        "  test_acc = clf.score(X_test, y_test)\n",
        "\n",
        "  table.append([i, clf.score(X_train, y_train), clf.score(X_test, y_test), clf.n_iter_])\n",
        "\n",
        "  avg_n_iter += n_iter\n",
        "  avg_train_acc += train_acc\n",
        "  avg_test_acc += test_acc\n",
        "\n",
        "  # print softmax probabilities of last trial (% confidence in each prediction)\n",
        "  if i == NUM_TRIALS - 1:\n",
        "    softmax(clf, X_test, y_test)\n",
        "\n",
        "table.append([\"Avg\", avg_train_acc / NUM_TRIALS, avg_test_acc / NUM_TRIALS, avg_n_iter / NUM_TRIALS])\n",
        "\n",
        "print(\"\\nResults:\")\n",
        "print(tabulate(table, headers=headers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL2Pt16a10OG",
        "outputId": "d888eaa1-f54a-4687-f042-9ea80eaafcbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP - Initial\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg certainty for given to correct vs each of the 2 incorrect classification:\n",
            "   Correct: 0.5692809878567462, Incorrect: 0.2153595060716271\n",
            "\n",
            "A few of the softmax probabilities using test data:\n",
            "[0.26421209 0.40202849 0.33375942] (0.0 actual)\n",
            "[0.57557068 0.18165394 0.24277539] (0.0 actual)\n",
            "[0.06805745 0.26337744 0.66856512] (2.0 actual)\n",
            "[0.22247661 0.39261629 0.3849071 ] (0.0 actual)\n",
            "[0.00432576 0.03030331 0.96537093] (2.0 actual)\n",
            "[0.56567711 0.24537977 0.18894313] (0.0 actual)\n",
            "[0.83503527 0.14036919 0.02459553] (0.0 actual)\n",
            "[0.023655   0.95534217 0.02100283] (1.0 actual)\n",
            "[0.02807646 0.0607433  0.91118024] (2.0 actual)\n",
            "[0.64188893 0.28976187 0.0683492 ] (0.0 actual)\n",
            "\n",
            "Results:\n",
            "Trial      Training Accuracy    Test Accuracy    Number of Iterations\n",
            "-------  -------------------  ---------------  ----------------------\n",
            "0                   0.556548         0.577381                    78\n",
            "1                   0.715476         0.70119                    183\n",
            "2                   0.733929         0.741667                   200\n",
            "3                   0.705952         0.695238                   200\n",
            "4                   0.680357         0.670238                   200\n",
            "5                   0.661607         0.625                       93\n",
            "6                   0.679762         0.67381                    136\n",
            "7                   0.719643         0.719048                   135\n",
            "8                   0.483036         0.475                       50\n",
            "9                   0.717262         0.7                        200\n",
            "Avg                 0.665357         0.657857                   147.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MLP - Improved\\n\")\n",
        "\n",
        "X, y = load_music_genre_classification(normalize=True)\n",
        "\n",
        "# Find the best parameters\n",
        "# clf = MLPClassifier(activation='logistic',alpha=0,early_stopping=True, n_iter_no_change=10, max_iter=500)\n",
        "# parameters = {'learning_rate_init':(.01, .1, 1),\n",
        "#               'hidden_layer_sizes': ([32], [64], [128]),\n",
        "#               'momentum':(0, .25, .5),\n",
        "#               # 'solver':('lbfgs', 'sgd', 'adam')\n",
        "#               }\n",
        "# grid = GridSearchCV(clf, parameters)\n",
        "# grid.fit(X_train,y_train) #This takes a while to run\n",
        "# print(grid.best_params_)\n",
        "# print(grid.best_score_)\n",
        "\n",
        "# Score with the bestparameters\n",
        "avg_train_score, avg_test_score = 0, 0\n",
        "for i in range(10):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
        "  # clf = MLPClassifier(activation='identity',alpha=0,early_stopping=True, n_iter_no_change=10, max_iter=500, learning_rate_init=0.01, momentum=0.25, hidden_layer_sizes=64)\n",
        "  clf = MLPClassifier(learning_rate_init=0.01, momentum=0.25, hidden_layer_sizes=64)\n",
        "  clf.fit(X_train, y_train)\n",
        "  avg_train_score += clf.score(X_train, y_train) / 10\n",
        "  avg_test_score += clf.score(X_test, y_test) / 10\n",
        "  print(f'Train_score: {clf.score(X_train, y_train)}')\n",
        "  print(f'   Test_score: {clf.score(X_test, y_test)}')\n",
        "\n",
        "print(f'Avg_train_score: {avg_train_score}')\n",
        "print(f'Avg_test_score: {avg_test_score}')\n"
      ],
      "metadata": {
        "id": "RLZ5ueeH7y06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d1ea4a9-2fdf-4328-9b4b-ed4b80180a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP - Improved\n",
            "\n",
            "Train_score: 0.8101190476190476\n",
            "   Test_score: 0.7833333333333333\n",
            "Train_score: 0.8113095238095238\n",
            "   Test_score: 0.7869047619047619\n",
            "Train_score: 0.8101190476190476\n",
            "   Test_score: 0.8071428571428572\n",
            "Train_score: 0.8139880952380952\n",
            "   Test_score: 0.7988095238095239\n",
            "Train_score: 0.819047619047619\n",
            "   Test_score: 0.7904761904761904\n",
            "Train_score: 0.8116071428571429\n",
            "   Test_score: 0.794047619047619\n",
            "Train_score: 0.8011904761904762\n",
            "   Test_score: 0.8130952380952381\n",
            "Train_score: 0.8151785714285714\n",
            "   Test_score: 0.7976190476190477\n",
            "Train_score: 0.8125\n",
            "   Test_score: 0.8\n",
            "Train_score: 0.8160714285714286\n",
            "   Test_score: 0.7892857142857143\n",
            "Avg_train_score: 0.8121130952380954\n",
            "Avg_test_score: 0.7960714285714285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"KNN - Initial\\n\")\n",
        "\n",
        "X, y = load_music_genre_classification(normalize=False)\n",
        "\n",
        "NUM_TRIALS = 10\n",
        "\n",
        "headers = [\"Trial\", \"Training Accuracy\", \"Test Accuracy\"]\n",
        "table = []\n",
        "\n",
        "avg_train_acc = 0.0\n",
        "avg_test_acc = 0.0\n",
        "for i in range(NUM_TRIALS):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
        "  knn = KNeighborsClassifier().fit(X_train, y_train)\n",
        "\n",
        "  train_acc = knn.score(X_train, y_train)\n",
        "  test_acc = knn.score(X_test, y_test)\n",
        "\n",
        "  avg_train_acc += train_acc\n",
        "  avg_test_acc += test_acc\n",
        "\n",
        "  table.append([i, train_acc, test_acc])\n",
        "\n",
        "  # print softmax probabilities of last trial (% confidence in each prediction)\n",
        "  if i == NUM_TRIALS - 1:\n",
        "    softmax(knn, X_test, y_test)\n",
        "\n",
        "table.append([\"Avg\", avg_train_acc / NUM_TRIALS, avg_test_acc / NUM_TRIALS])\n",
        "\n",
        "print()\n",
        "print(tabulate(table, headers=headers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5S_cp7a58YD",
        "outputId": "5590523c-3cfa-4c6e-9f99-5c8e4d404b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN - Initial\n",
            "\n",
            "Avg certainty for given to correct vs each of the 2 incorrect classification:\n",
            "   Correct: 0.42166666666666575, Incorrect: 0.28916666666666385\n",
            "\n",
            "A few of the softmax probabilities using test data:\n",
            "[0.6 0.4 0. ] (0.0 actual)\n",
            "[1. 0. 0.] (0.0 actual)\n",
            "[0.  0.2 0.8] (2.0 actual)\n",
            "[0.4 0.2 0.4] (0.0 actual)\n",
            "[0.2 0.2 0.6] (2.0 actual)\n",
            "[0.2 0.2 0.6] (0.0 actual)\n",
            "[0.4 0.2 0.4] (0.0 actual)\n",
            "[0.  0.8 0.2] (1.0 actual)\n",
            "[0.2 0.2 0.6] (2.0 actual)\n",
            "[0.4 0.4 0.2] (0.0 actual)\n",
            "\n",
            "Trial      Training Accuracy    Test Accuracy\n",
            "-------  -------------------  ---------------\n",
            "0                   0.637798         0.477381\n",
            "1                   0.637798         0.466667\n",
            "2                   0.635417         0.463095\n",
            "3                   0.637202         0.482143\n",
            "4                   0.644345         0.453571\n",
            "5                   0.645536         0.439286\n",
            "6                   0.648214         0.45119\n",
            "7                   0.644048         0.447619\n",
            "8                   0.632143         0.49881\n",
            "9                   0.639286         0.471429\n",
            "Avg                 0.640179         0.465119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"KNN - Improved\\n\")\n",
        "\n",
        "X, y = load_music_genre_classification(normalize=True)\n",
        "\n",
        "headers = [\"n_neighbors\", \"Avg Training Accuracy\", \"Avg Test Accuracy\"]\n",
        "table = []\n",
        "\n",
        "NUM_TRIALS = 10\n",
        "\n",
        "n_neighbors_trials = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 30, 40, 50, 75, 100, 150, 200]\n",
        "for n_neighbors in n_neighbors_trials:\n",
        "  row = [n_neighbors, 0, 0, 0]\n",
        "  for i in range(NUM_TRIALS):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
        "    knn = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X_train, y_train)\n",
        "\n",
        "    row[1] += knn.score(X_train, y_train)\n",
        "    row[2] += knn.score(X_test, y_test)\n",
        "\n",
        "  table.append(row)\n",
        "\n",
        "for i in range(len(table)):\n",
        "  for j in range(1, 3):\n",
        "    table[i][j] /= NUM_TRIALS\n",
        "\n",
        "print(tabulate(table, headers=headers))"
      ],
      "metadata": {
        "id": "cDJPHEu071BN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab2453f-ac2b-4e42-f8a9-349cc11bb694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN - Improved\n",
            "\n",
            "       n_neighbors    Avg Training Accuracy    Avg Test Accuracy\n",
            "---  -------------  -----------------------  -------------------\n",
            "  1       0.999286                 0.698095                    0\n",
            "  2       0.849077                 0.701548                    0\n",
            "  3       0.84872                  0.73881                     0\n",
            "  4       0.827857                 0.747143                    0\n",
            "  5       0.82753                  0.76                        0\n",
            "  6       0.81869                  0.760952                    0\n",
            "  7       0.82                     0.769048                    0\n",
            "  8       0.816071                 0.768095                    0\n",
            "  9       0.816339                 0.770476                    0\n",
            " 10       0.814792                 0.774405                    0\n",
            " 15       0.804911                 0.773571                    0\n",
            " 20       0.800833                 0.776905                    0\n",
            " 30       0.793601                 0.774762                    0\n",
            " 40       0.788363                 0.77631                     0\n",
            " 50       0.789702                 0.775                       0\n",
            " 75       0.78378                  0.774643                    0\n",
            "100       0.781964                 0.772143                    0\n",
            "150       0.779821                 0.77                        0\n",
            "200       0.776131                 0.77131                     0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DT - Initial\\n\")\n",
        "\n",
        "X, y = load_music_genre_classification(normalize=False)\n",
        "\n",
        "NUM_TRIALS = 10\n",
        "\n",
        "headers = [\"Trial\", \"Training Accuracy\", \"Test Accuracy\"]\n",
        "table = []\n",
        "\n",
        "avg_train_acc, avg_test_acc = 0, 0\n",
        "for i in range(NUM_TRIALS):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
        "  clf = DecisionTreeClassifier(random_state=i).fit(X_train, y_train)\n",
        "\n",
        "  train_acc = clf.score(X_train, y_train)\n",
        "  test_acc = clf.score(X_test, y_test)\n",
        "\n",
        "  avg_train_acc += train_acc\n",
        "  avg_test_acc += test_acc\n",
        "\n",
        "  table.append([i, train_acc, test_acc])\n",
        "\n",
        "  # print softmax probabilities of last trial (% confidence in each prediction)\n",
        "  if i == NUM_TRIALS - 1:\n",
        "    softmax(clf, X_test, y_test)\n",
        "\n",
        "table.append([\"Avg\", avg_train_acc / NUM_TRIALS, avg_test_acc / NUM_TRIALS])\n",
        "\n",
        "print()\n",
        "print(tabulate(table, headers=headers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaKEIfVV5_tO",
        "outputId": "c7522865-22c6-4a5d-bff4-17b4f716ef9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DT - Initial\n",
            "\n",
            "Avg certainty for given to correct vs each of the 2 incorrect classification:\n",
            "   Correct: 0.7095238095238096, Incorrect: 0.14523809523809525\n",
            "\n",
            "A few of the softmax probabilities using test data:\n",
            "[0. 0. 1.] (0.0 actual)\n",
            "[1. 0. 0.] (0.0 actual)\n",
            "[0. 0. 1.] (2.0 actual)\n",
            "[0. 0. 1.] (0.0 actual)\n",
            "[0. 0. 1.] (2.0 actual)\n",
            "[0. 1. 0.] (0.0 actual)\n",
            "[1. 0. 0.] (0.0 actual)\n",
            "[0. 1. 0.] (1.0 actual)\n",
            "[0. 0. 1.] (2.0 actual)\n",
            "[1. 0. 0.] (0.0 actual)\n",
            "\n",
            "Trial      Training Accuracy    Test Accuracy\n",
            "-------  -------------------  ---------------\n",
            "0                   0.999107         0.695238\n",
            "1                   0.999405         0.705952\n",
            "2                   0.999107         0.690476\n",
            "3                   0.999107         0.722619\n",
            "4                   0.999702         0.678571\n",
            "5                   0.999405         0.670238\n",
            "6                   0.999702         0.720238\n",
            "7                   0.99881          0.716667\n",
            "8                   0.999405         0.697619\n",
            "9                   0.999107         0.709524\n",
            "Avg                 0.999286         0.700714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DT - Improved\\n\")\n",
        "\n",
        "X, y = load_music_genre_classification(normalize=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# criterion and min split and max depth\n",
        "\n",
        "\n",
        "# # criterion - gini best\n",
        "# headers = [\"criterion\", \"Avg Training Accuracy\", \"Avg Test Accuracy\"]\n",
        "# table = []\n",
        "\n",
        "# NUM_TRIALS = 10\n",
        "\n",
        "# criterions = ['gini', 'entropy', 'log_loss']\n",
        "# for criterion in criterions:\n",
        "#   row = [criterion, 0, 0]\n",
        "#   for i in range(NUM_TRIALS):\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
        "#     clf = DecisionTreeClassifier(random_state=i, criterion=criterion).fit(X_train, y_train)\n",
        "\n",
        "#     row[1] += clf.score(X_train, y_train)\n",
        "#     row[2] += clf.score(X_test, y_test)\n",
        "\n",
        "#   table.append(row)\n",
        "\n",
        "# for i in range(len(table)):\n",
        "#   for j in range(1, 3):\n",
        "#     table[i][j] /= NUM_TRIALS\n",
        "\n",
        "# print(tabulate(table, headers=headers))\n",
        "# # criterion      Avg Training Accuracy    Avg Test Accuracy\n",
        "# # -----------  -----------------------  -------------------\n",
        "# # gini                        0.999286             0.700357\n",
        "# # entropy                     0.999286             0.696786\n",
        "# # log_loss                    0.999286             0.696786\n",
        "\n",
        "\n",
        "# # min_samples_split - 50 best\n",
        "# headers = [\"min__samples split\", \"Avg Training Accuracy\", \"Avg Test Accuracy\"]\n",
        "# table = []\n",
        "\n",
        "# NUM_TRIALS = 10\n",
        "\n",
        "# min_samples_split_trials = [2, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "# for min_samples_split in min_samples_split_trials:\n",
        "#   row = [min_samples_split, 0, 0]\n",
        "#   for i in range(NUM_TRIALS):\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
        "#     clf = DecisionTreeClassifier(random_state=i, criterion='gini', min_samples_split=min_samples_split).fit(X_train, y_train)\n",
        "\n",
        "#     row[1] += clf.score(X_train, y_train)\n",
        "#     row[2] += clf.score(X_test, y_test)\n",
        "\n",
        "#   table.append(row)\n",
        "\n",
        "# for i in range(len(table)):\n",
        "#   for j in range(1, 3):\n",
        "#     table[i][j] /= NUM_TRIALS\n",
        "\n",
        "# print(tabulate(table, headers=headers))\n",
        "#   min__samples split    Avg Training Accuracy    Avg Test Accuracy\n",
        "# --------------------  -----------------------  -------------------\n",
        "#                    2                 0.999286             0.700357\n",
        "#                    5                 0.960536             0.704643\n",
        "#                   10                 0.913601             0.717262\n",
        "#                   20                 0.867262             0.729048\n",
        "#                   30                 0.844821             0.735119\n",
        "#                   40                 0.827827             0.739643\n",
        "#                   50                 0.814673             0.743571 # BEST\n",
        "#                   60                 0.803571             0.740238\n",
        "#                   70                 0.795476             0.74\n",
        "#                   80                 0.789315             0.739048\n",
        "#                   90                 0.782232             0.736786\n",
        "#                  100                 0.777738             0.736429\n",
        "\n",
        "\n",
        "# # max depth - 10 best\n",
        "# headers = [\"max_depth\", \"Avg Training Accuracy\", \"Avg Test Accuracy\"]\n",
        "# table = []\n",
        "\n",
        "# NUM_TRIALS = 10\n",
        "\n",
        "# max_depths = [2, 3, 5, 10, 15, 20, 30, 40, 50, 100, 150, 200, None]\n",
        "# for max_depth in max_depths:\n",
        "#   row = [max_depth, 0, 0]\n",
        "#   for i in range(NUM_TRIALS):\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
        "#     clf = DecisionTreeClassifier(random_state=i, criterion='gini', min_samples_split=50, max_depth=max_depth).fit(X_train, y_train)\n",
        "\n",
        "#     row[1] += clf.score(X_train, y_train)\n",
        "#     row[2] += clf.score(X_test, y_test)\n",
        "\n",
        "#   table.append(row)\n",
        "\n",
        "# for i in range(len(table)):\n",
        "#   for j in range(1, 3):\n",
        "#     table[i][j] /= NUM_TRIALS\n",
        "\n",
        "# print(tabulate(table, headers=headers))\n",
        "# #   max_depth    Avg Training Accuracy    Avg Test Accuracy\n",
        "# # -----------  -----------------------  -------------------\n",
        "# #           2                 0.636726             0.626071\n",
        "# #           3                 0.67247              0.651905\n",
        "# #           5                 0.761786             0.7225\n",
        "# #          10                 0.813869             0.745833\n",
        "# #          15                 0.814673             0.743571\n",
        "# #          20                 0.814673             0.743571\n",
        "# #          30                 0.814673             0.743571\n",
        "# #          40                 0.814673             0.743571\n",
        "# #          50                 0.814673             0.743571\n",
        "# #         100                 0.814673             0.743571\n",
        "# #         150                 0.814673             0.743571\n",
        "# #         200                 0.814673             0.743571\n",
        "# #                             0.814673             0.743571"
      ],
      "metadata": {
        "id": "qsO0oKk07-or",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60e9c14c-9d03-4e5c-b0c4-1ac213128e4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DT - Improved\n",
            "\n",
            "  max_depth    Avg Training Accuracy    Avg Test Accuracy\n",
            "-----------  -----------------------  -------------------\n",
            "          2                 0.636726             0.626071\n",
            "          3                 0.67247              0.651905\n",
            "          5                 0.761786             0.7225\n",
            "         10                 0.813869             0.745833\n",
            "         15                 0.814673             0.743571\n",
            "         20                 0.814673             0.743571\n",
            "         30                 0.814673             0.743571\n",
            "         40                 0.814673             0.743571\n",
            "         50                 0.814673             0.743571\n",
            "        100                 0.814673             0.743571\n",
            "        150                 0.814673             0.743571\n",
            "        200                 0.814673             0.743571\n",
            "                            0.814673             0.743571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Notes: It is a bagging algorithm\n",
        "\n",
        "1. Build a tree\n",
        "    1. Create random dataset from the original, the same size as the original (some samples will be repeats)\n",
        "    2. At each split, only consider n randomly selected features (columns) (rather than considering all)  \n",
        "2. Repeat 100's of times, creating 100's of trees  \n",
        "3. To classify\n",
        "    1. Classify individually with each DT\n",
        "    2. The classification with the most votes wins\n",
        "\n",
        "To Improve Performance:\n",
        "- Test with different nums of features to consider at each split"
      ],
      "metadata": {
        "id": "J0e3y6VYcVZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"RF - Initial\\n\")\n",
        "\n",
        "X, y = load_music_genre_classification(normalize=False)\n",
        "\n",
        "NUM_TRIALS = 10\n",
        "\n",
        "headers = [\"Trial\", \"Training Accuracy\", \"Test Accuracy\"]\n",
        "table = []\n",
        "\n",
        "avg_train_acc, avg_test_acc = 0, 0\n",
        "for i in range(NUM_TRIALS):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
        "  clf = RandomForestClassifier(random_state=i).fit(X_train, y_train)\n",
        "\n",
        "  train_acc = clf.score(X_train, y_train)\n",
        "  test_acc = clf.score(X_test, y_test)\n",
        "\n",
        "  avg_train_acc += train_acc\n",
        "  avg_test_acc += test_acc\n",
        "\n",
        "  table.append([i, train_acc, test_acc])\n",
        "\n",
        "  # print softmax probabilities of last trial (% confidence in each prediction)\n",
        "  if i == NUM_TRIALS - 1:\n",
        "    softmax(clf, X_test, y_test)\n",
        "\n",
        "table.append([\"Avg\", avg_train_acc / NUM_TRIALS, avg_test_acc / NUM_TRIALS])\n",
        "\n",
        "print()\n",
        "print(tabulate(table, headers=headers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuMu3H8E6BTx",
        "outputId": "1c2a901f-0719-42f8-94a9-f258d1c497be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF - Initial\n",
            "\n",
            "Avg certainty for given to correct vs each of the 2 incorrect classification:\n",
            "   Correct: 0.6684238095238104, Incorrect: 0.1657880952380952\n",
            "\n",
            "A few of the softmax probabilities using test data:\n",
            "[0.21 0.39 0.4 ] (0.0 actual)\n",
            "[0.6  0.06 0.34] (0.0 actual)\n",
            "[0.01 0.07 0.92] (2.0 actual)\n",
            "[0.53 0.18 0.29] (0.0 actual)\n",
            "[0.01 0.18 0.81] (2.0 actual)\n",
            "[0.51 0.45 0.04] (0.0 actual)\n",
            "[0.83 0.1  0.07] (0.0 actual)\n",
            "[0.04 0.94 0.02] (1.0 actual)\n",
            "[0. 0. 1.] (2.0 actual)\n",
            "[0.94 0.06 0.  ] (0.0 actual)\n",
            "\n",
            "Trial      Training Accuracy    Test Accuracy\n",
            "-------  -------------------  ---------------\n",
            "0                   0.99881          0.77619\n",
            "1                   0.999405         0.786905\n",
            "2                   0.999107         0.796429\n",
            "3                   0.999107         0.788095\n",
            "4                   0.999702         0.772619\n",
            "5                   0.999405         0.766667\n",
            "6                   0.999702         0.783333\n",
            "7                   0.99881          0.791667\n",
            "8                   0.999405         0.79881\n",
            "9                   0.999107         0.777381\n",
            "Avg                 0.999256         0.78381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"RF - Improved\\n\")\n",
        "\n",
        "X, y = load_music_genre_classification(normalize=True)\n",
        "\n",
        "\n",
        "# # criterion - entropy best\n",
        "# headers = [\"criterion\", \"Avg Training Accuracy\", \"Avg Test Accuracy\"]\n",
        "# table = []\n",
        "\n",
        "# NUM_TRIALS = 10\n",
        "\n",
        "# criterions = ['gini', 'entropy', 'log_loss']\n",
        "# for criterion in criterions:\n",
        "#   row = [criterion, 0, 0]\n",
        "#   for i in range(NUM_TRIALS):\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
        "#     clf = RandomForestClassifier(random_state=i, criterion=criterion).fit(X_train, y_train)\n",
        "\n",
        "#     row[1] += clf.score(X_train, y_train)\n",
        "#     row[2] += clf.score(X_test, y_test)\n",
        "\n",
        "#   table.append(row)\n",
        "\n",
        "# for i in range(len(table)):\n",
        "#   for j in range(1, 3):\n",
        "#     table[i][j] /= NUM_TRIALS\n",
        "\n",
        "# print(tabulate(table, headers=headers))\n",
        "# # criterion      Avg Training Accuracy    Avg Test Accuracy\n",
        "# # -----------  -----------------------  -------------------\n",
        "# # gini                        0.999256             0.783929\n",
        "# # entropy                     0.999286             0.785 #BEST\n",
        "# # log_loss                    0.999286             0.785\n",
        "\n",
        "\n",
        "# # min_samples_split - 10 best\n",
        "# headers = [\"min__samples split\", \"Avg Training Accuracy\", \"Avg Test Accuracy\"]\n",
        "# table = []\n",
        "\n",
        "# NUM_TRIALS = 10\n",
        "\n",
        "# min_samples_split_trials = [2, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "# for min_samples_split in min_samples_split_trials:\n",
        "#   row = [min_samples_split, 0, 0]\n",
        "#   for i in range(NUM_TRIALS):\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
        "#     clf = RandomForestClassifier(random_state=i, criterion='entropy', min_samples_split=min_samples_split).fit(X_train, y_train)\n",
        "\n",
        "#     row[1] += clf.score(X_train, y_train)\n",
        "#     row[2] += clf.score(X_test, y_test)\n",
        "\n",
        "#   table.append(row)\n",
        "\n",
        "# for i in range(len(table)):\n",
        "#   for j in range(1, 3):\n",
        "#     table[i][j] /= NUM_TRIALS\n",
        "\n",
        "# print(tabulate(table, headers=headers))\n",
        "# #   min__samples split    Avg Training Accuracy    Avg Test Accuracy\n",
        "# # --------------------  -----------------------  -------------------\n",
        "# #                    2                 0.999286             0.785\n",
        "# #                    5                 0.991815             0.78369\n",
        "# #                   10                 0.9475               0.784762 # BEST\n",
        "# #                   20                 0.891905             0.782976\n",
        "# #                   30                 0.865595             0.779881\n",
        "# #                   40                 0.848095             0.778214\n",
        "# #                   50                 0.835446             0.776548\n",
        "# #                   60                 0.827024             0.77381\n",
        "# #                   70                 0.82                 0.770357\n",
        "# #                   80                 0.812887             0.768214\n",
        "# #                   90                 0.808452             0.767976\n",
        "# #                  100                 0.803333             0.76369\n",
        "\n",
        "\n",
        "# # max depth - 11 best\n",
        "# headers = [\"max_depth\", \"Avg Training Accuracy\", \"Avg Test Accuracy\"]\n",
        "# table = []\n",
        "\n",
        "# NUM_TRIALS = 10\n",
        "\n",
        "# max_depths = [2, 3, 5, 10, 11, 12, 13, 14, 15, 20, 30, 40, 50, 100, 150, 200, None]\n",
        "# for max_depth in max_depths:\n",
        "#   row = [max_depth, 0, 0]\n",
        "#   for i in range(NUM_TRIALS):\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
        "#     clf = RandomForestClassifier(random_state=i, criterion='entropy', min_samples_split=10, max_depth=max_depth).fit(X_train, y_train)\n",
        "\n",
        "#     row[1] += clf.score(X_train, y_train)\n",
        "#     row[2] += clf.score(X_test, y_test)\n",
        "\n",
        "#   table.append(row)\n",
        "\n",
        "# for i in range(len(table)):\n",
        "#   for j in range(1, 3):\n",
        "#     table[i][j] /= NUM_TRIALS\n",
        "\n",
        "# print(tabulate(table, headers=headers))\n",
        "# #   max_depth    Avg Training Accuracy    Avg Test Accuracy\n",
        "# # -----------  -----------------------  -------------------\n",
        "# #           2                 0.72497              0.7075\n",
        "# #           3                 0.75122              0.731071\n",
        "# #           5                 0.796875             0.758095\n",
        "# #          10                 0.911399             0.783333\n",
        "# #          11                 0.924315             0.7825\n",
        "# #          12                 0.93494              0.785357 # BEST\n",
        "# #          13                 0.939583             0.783571\n",
        "# #          14                 0.943363             0.784405\n",
        "# #          15                 0.944613             0.783452\n",
        "# #          20                 0.947619             0.785\n",
        "# #          30                 0.9475               0.784762\n",
        "# #          40                 0.9475               0.784762\n",
        "# #          50                 0.9475               0.784762\n",
        "# #         100                 0.9475               0.784762\n",
        "# #         150                 0.9475               0.784762\n",
        "# #         200                 0.9475               0.784762\n",
        "# #                             0.9475               0.784762"
      ],
      "metadata": {
        "id": "FU-13hbv8Ai4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ee5bcf8-31b4-4b5c-d312-820cc9424ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF - Improved\n",
            "\n",
            "  max_depth    Avg Training Accuracy    Avg Test Accuracy\n",
            "-----------  -----------------------  -------------------\n",
            "          2                 0.72497              0.7075\n",
            "          3                 0.75122              0.731071\n",
            "          5                 0.796875             0.758095\n",
            "         10                 0.911399             0.783333\n",
            "         11                 0.924315             0.7825\n",
            "         12                 0.93494              0.785357\n",
            "         13                 0.939583             0.783571\n",
            "         14                 0.943363             0.784405\n",
            "         15                 0.944613             0.783452\n",
            "         20                 0.947619             0.785\n",
            "         30                 0.9475               0.784762\n",
            "         40                 0.9475               0.784762\n",
            "         50                 0.9475               0.784762\n",
            "        100                 0.9475               0.784762\n",
            "        150                 0.9475               0.784762\n",
            "        200                 0.9475               0.784762\n",
            "                            0.9475               0.784762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Naive Bayes - Initial\\n\")\n",
        "\n",
        "X, y = load_music_genre_classification(normalize=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "NUM_TRIALS = 10\n",
        "\n",
        "headers = [\"Trial\", \"Training Accuracy\", \"Test Accuracy\"]\n",
        "table = []\n",
        "\n",
        "avg_train_acc, avg_test_acc = 0, 0\n",
        "for i in range(NUM_TRIALS):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
        "  clf = GaussianNB().fit(X_train, y_train)\n",
        "\n",
        "  train_acc = clf.score(X_train, y_train)\n",
        "  test_acc = clf.score(X_test, y_test)\n",
        "\n",
        "  avg_train_acc += train_acc\n",
        "  avg_test_acc += test_acc\n",
        "\n",
        "  table.append([i, train_acc, test_acc])\n",
        "\n",
        "  # print softmax probabilities of last trial (% confidence in each prediction)\n",
        "  if i == NUM_TRIALS - 1:\n",
        "    softmax(clf, X_test, y_test)\n",
        "\n",
        "table.append([\"Avg\", avg_train_acc / NUM_TRIALS, avg_test_acc / NUM_TRIALS])\n",
        "\n",
        "print()\n",
        "print(tabulate(table, headers=headers))"
      ],
      "metadata": {
        "id": "zJaaE58n6o50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461984cb-2a03-4f76-b37f-f8d007819335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes - Initial\n",
            "\n",
            "Avg certainty for given to correct vs each of the 2 incorrect classification:\n",
            "   Correct: 0.6530883686724849, Incorrect: 0.17345581566375812\n",
            "\n",
            "A few of the softmax probabilities using test data:\n",
            "[0.34996155 0.04158794 0.60845051] (0.0 actual)\n",
            "[0.84176932 0.01749257 0.14073811] (0.0 actual)\n",
            "[0.01964259 0.01590244 0.96445497] (2.0 actual)\n",
            "[0.2583926  0.05251889 0.68908851] (0.0 actual)\n",
            "[5.96508293e-09 6.23034224e-05 9.99937691e-01] (2.0 actual)\n",
            "[0.46700475 0.05173814 0.48125712] (0.0 actual)\n",
            "[0.94915722 0.02780131 0.02304146] (0.0 actual)\n",
            "[7.96622853e-007 9.99999203e-001 2.52020969e-128] (1.0 actual)\n",
            "[3.74598303e-04 1.52096724e-03 9.98104434e-01] (2.0 actual)\n",
            "[0.90006141 0.05491175 0.04502684] (0.0 actual)\n",
            "\n",
            "Trial      Training Accuracy    Test Accuracy\n",
            "-------  -------------------  ---------------\n",
            "0                   0.646429         0.672619\n",
            "1                   0.684524         0.659524\n",
            "2                   0.645536         0.638095\n",
            "3                   0.681548         0.677381\n",
            "4                   0.688393         0.7\n",
            "5                   0.6875           0.670238\n",
            "6                   0.646726         0.653571\n",
            "7                   0.683631         0.680952\n",
            "8                   0.683631         0.695238\n",
            "9                   0.685417         0.690476\n",
            "Avg                 0.673333         0.67381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Naive Bayes - Improved\\n\")\n",
        "\n",
        "X, y = load_music_genre_classification(normalize=False)\n",
        "\n",
        "\n",
        "headers = [\"var_smoothing\", \"Avg Training Accuracy\", \"Avg Test Accuracy\"]\n",
        "table = []\n",
        "\n",
        "NUM_TRIALS = 10\n",
        "\n",
        "var_smoothing_trials = [0.00001, 0.000001, 0.0000001, 0.0000005, 0.00000001, 0.000000015, 0.000000001, 0.0000000001] # 1e-5 to 1e-10\n",
        "for var_smoothing in var_smoothing_trials:\n",
        "  row = [var_smoothing, 0, 0]\n",
        "  for i in range(NUM_TRIALS):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
        "    clf = GaussianNB(var_smoothing=var_smoothing).fit(X_train, y_train)\n",
        "\n",
        "    row[1] += clf.score(X_train, y_train)\n",
        "    row[2] += clf.score(X_test, y_test)\n",
        "\n",
        "  table.append(row)\n",
        "\n",
        "for i in range(len(table)):\n",
        "  for j in range(1, 3):\n",
        "    table[i][j] /= NUM_TRIALS\n",
        "\n",
        "print(tabulate(table, headers=headers))"
      ],
      "metadata": {
        "id": "XgAKoM-l64Rn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d39141cb-b925-44a3-bde9-3674fe2cdd0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes - Improved\n",
            "\n",
            "  var_smoothing    Avg Training Accuracy    Avg Test Accuracy\n",
            "---------------  -----------------------  -------------------\n",
            "        1e-05                   0.553393             0.55119\n",
            "        1e-06                   0.628036             0.626548\n",
            "        1e-07                   0.717649             0.719048\n",
            "        5e-07                   0.658036             0.65631\n",
            "        1e-08                   0.745774             0.741071\n",
            "        1.5e-08                 0.745476             0.743452\n",
            "        1e-09                   0.724821             0.723214\n",
            "        1e-10                   0.694464             0.693214\n"
          ]
        }
      ]
    }
  ]
}